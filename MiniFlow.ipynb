{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Url: http://172.16.51.240:8000/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- http://jupyter.readthedocs.io/en/latest/install.html\n",
    "\n",
    "```\n",
    "$ pip install jupyter\n",
    "$ pip install numpy\n",
    "$ pip install matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniflow - A simple python code that imitates Tensorflow \n",
    "\n",
    "**Disclaimer:** This is pure and so called original work of **Ctrl + C** and **Ctrl + V** ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "- Supervised\n",
    "- Unsupervised\n",
    "- Semi Supervised\n",
    "\n",
    "```\n",
    "                                            ---------------- \n",
    "                                           |                |\n",
    "                                           v                |\n",
    "Data -> Preprocessing -> Batching -> Model Training -> Evaluation -> Required Accuracy? -> Deploy\n",
    "\n",
    "\n",
    "### Model Trianing (Supervised)\n",
    "\n",
    "(Input Features, Target(s)) -> Intialize Model      ->    Predict the Target ->      Calculate the Error\n",
    "                               Parameters Randomly                       ^                            |                                                                              |                            |\n",
    "                                                                         |                            |\n",
    "                                                                         |                            |\n",
    "                                                                         |                            |\n",
    "                                                                          -------Update Parameters----\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets start with an demo \n",
    "- http://playground.tensorflow.org/\n",
    "- https://www.mathway.com/Algebra\n",
    "- http://setosa.io/ev/ordinary-least-squares-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Jargons:\n",
    "- Input Layer\n",
    "- Output Layer\n",
    "- Weights\n",
    "- Bias\n",
    "- Forward propagation\n",
    "- Activation Function - Sigmoid\n",
    "- Backward propagation\n",
    "- Error Calculation\n",
    "- Regularization\n",
    "- Gradient Descent\n",
    "\n",
    "![](https://qph.ec.quoracdn.net/main-qimg-5285e8d35c8cf10009d9672d7b2c5ac9-c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Outline:\n",
    "- Abstract a simple Neural Node/[Neuron](https://en.wikipedia.org/wiki/Neuron)\n",
    "    - With some I/O properties\n",
    "    - How to do a forward pass\n",
    "    - How to calculate gradients and do backward pass\n",
    "- Create an Input Node\n",
    "- Create some operations like Add, Multiply, Liner Tranformation, Sigmoid, Mean Square Error\n",
    "- Explore how forward pass is done in all the operation\n",
    "- Learn Mathematics behind Backpropagation (Don't worry even I am scared!)\n",
    "- Get to see how gradients are caculated\n",
    "- Explore how it is implemented \n",
    "- SGD - A simple algorithm to adapt teh weights for descent predictions\n",
    "\n",
    "![](w2-backprop-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Modeling vs Software Designing\n",
    "\n",
    "\n",
    "\"Modeling\" is describing something you know. A good model makes correct assertions. Expressing a scientific theory or algorithm in software. \n",
    "\n",
    "Software design is the process of defining software methods, functions, objects, and the overall structure and interaction of your code so that the resulting functionality will satisfy your users requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing  Neural Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Base class for nodes in the network.\n",
    "    Should have following properties:\n",
    "    1. Should hold its value\n",
    "    2. Should know what are incoming nodes\n",
    "    3. Should know to which node(s) it outputs the value\n",
    "    4. Should hold the gradient calculated\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_nodes`: A list of nodes with edges into this node.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"\n",
    "        Node's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all nodes need.\n",
    "        \"\"\"\n",
    "        self.name = \"Node\"\n",
    "        \n",
    "        # The eventual value of this node. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        \n",
    "        # A list of nodes with edges into this node.\n",
    "        # Just like input arguments to any function/method\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "\n",
    "        # A list of nodes that this node outputs to.\n",
    "        # Is it possible to know which node I am gonna send the result? Definelty NO!!!\n",
    "        self.outbound_nodes = []\n",
    "        \n",
    "        # Keys are the inputs to this node and\n",
    "        # their values are the partials of this node with\n",
    "        # respect to that input.\n",
    "        self.gradients = {}\n",
    "        \n",
    "        # Sets this node as an outbound node for all of\n",
    "        # this node's inputs.\n",
    "        # Hey there I am your output node, do send me your results, ok!\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `backward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, name='Input'):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        Node.__init__(self)\n",
    "        self.name = name\n",
    "\n",
    "    # NOTE: Input node is the only node where the value\n",
    "    # may be passed as an argument to forward().\n",
    "    #\n",
    "    # All other node implementations should get the value\n",
    "    # of the previous node from self.inbound_nodes\n",
    "    #\n",
    "    # Example:\n",
    "    # val0 = self.inbound_nodes[0].value\n",
    "    def forward(self, value=None):\n",
    "        # Overwrite the value if one is passed in.\n",
    "        if(DEBUG) : print(\"\\n----->Forward pass @ \" ,self.name)\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "            if(DEBUG) : print(\"w.r.t {} node of value: {} \".format(self.name, self.value))\n",
    "            \n",
    "\n",
    "    def backward(self):\n",
    "        # An Input node has no inputs so the gradient (derivative)\n",
    "        # is zero.\n",
    "        # The key, `self`, is reference to this object.\n",
    "        self.gradients = {self: 0}\n",
    "        # Weights and bias may be inputs, so you need to sum\n",
    "        # the gradient from output gradients.\n",
    "        if(DEBUG) : print('\\n')\n",
    "        if(DEBUG) : print('=============================\\n\\tBP @ {}\\n=============================\\n'.format(self.name))\n",
    "        if(DEBUG) : print('Initial Gradients:\\n------------------')\n",
    "        if(DEBUG) : print('W.r.t {}: \\n------------\\n{}'.format(self.name,self.gradients[self]))\n",
    "            \n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            \n",
    "            if(DEBUG) : print('\\n')\n",
    "            if(DEBUG) : print('Getting ', n.name, 'gradient : \\n<-----------------------------\\n', grad_cost)\n",
    "            if(DEBUG) : print('\\n')\n",
    "                \n",
    "            self.gradients[self] += grad_cost * 1\n",
    "            \n",
    "        if(DEBUG) : print('Calculated Final Gradient:(Note: Calculated by next node in the graph!!!)\\n----------------')\n",
    "        if(DEBUG) : print('W.r.t ', self.name, ' : \\n-------------\\n', self.gradients[self])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAG\n",
    "- https://en.wikipedia.org/wiki/Directed_acyclic_graph\n",
    "- https://stackoverflow.com/questions/2283757/can-someone-explain-in-simple-terms-to-me-what-a-directed-acyclic-graph-is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Topological sorting:** \n",
    "- https://en.wikipedia.org/wiki/Topological_sorting  \n",
    "- http://www.geeksforgeeks.org/topological-sorting/ (oh ya I need to keep this in my interview preparation!)  \n",
    "    In order to define your network, you'll need to define the order of operations for your nodes. Given that the input to some node depends on the outputs of others, you need to flatten the graph in such a way where all the input dependencies for each node are resolved before trying to run its calculation. This is a technique called a topological sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T_DEBUG = True\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is \n",
    "    the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "    if T_DEBUG: print('-----> topological_sort')\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    \n",
    "    if T_DEBUG: print('Input Nodes:'); [print(n.name) for n in input_nodes]\n",
    "        \n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        \n",
    "        if T_DEBUG: print('Pop: ', n.name)\n",
    "            \n",
    "        if n not in G:\n",
    "            if T_DEBUG: print('Adding: ', n.name, 'to the Graph')\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "            \n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G: \n",
    "                if T_DEBUG: print('Adding: ', m.name, 'to the Graph')\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "                \n",
    "            G[n]['out'].add(m); \n",
    "            if T_DEBUG: print('Adding', n.name, '----->', m.name)\n",
    "                \n",
    "            G[m]['in'].add(n); \n",
    "            if T_DEBUG: print('Adding', m.name, '<-----', n.name)\n",
    "                \n",
    "            nodes.append(m)\n",
    "            if T_DEBUG: print('Appending ', m.name)\n",
    "          \n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    if T_DEBUG: print('Input Nodes:'); [print(n.name) for n in S]\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "        if T_DEBUG: print('Pop: ', n.name)\n",
    "\n",
    "        #Assign values to the input node\n",
    "        if isinstance(n, Input):\n",
    "            if T_DEBUG: print('Feeding value: ', feed_dict[n], ' =====>  ', n.name)\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        if T_DEBUG: print('Adding ', n.name, 'to the sorted List')\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            if T_DEBUG: print('Removing', n.name, '----->', m.name)\n",
    "            if T_DEBUG: print('Removing', m.name, '<-----', n.name)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                if T_DEBUG: print('\\nNo input nodes!!! Adding: ', m.name, 'to the Graph\\n')\n",
    "                S.add(m)\n",
    "    \n",
    "    if T_DEBUG: print('Sorted Nodes:\\n'); [print(n.name) for n in L]\n",
    "    \n",
    "    if T_DEBUG: print('<------------------------------------ topological_sort')\n",
    "        \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: A node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: A topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output Node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets define our Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Add(Node):\n",
    "    def __init__(self, *inputs):\n",
    "        Node.__init__(self, inputs)\n",
    "        self.name = \"Add_Op\"\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        For reference, here's the old way from the last\n",
    "        quiz. You'll want to write code here.\n",
    "        \"\"\"\n",
    "        self.value = 0\n",
    "        for i in range(len(self.inbound_nodes)):\n",
    "            \n",
    "            if(DEBUG) : print(\"Initial value of {} is {}\".format(self.name, self.value))\n",
    "                \n",
    "            self.value +=  self.inbound_nodes[i].value\n",
    "            \n",
    "            if(DEBUG) : print(\"{}:{} ---> {}:{}\".format(self.inbound_nodes[i].name, self.inbound_nodes[i].value, \n",
    "                                           self.name, self.value))\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> topological_sort\n",
      "Input Nodes:\n",
      "x\n",
      "y\n",
      "z\n",
      "Pop:  x\n",
      "Adding:  x to the Graph\n",
      "Adding:  Add_Op to the Graph\n",
      "Adding x -----> Add_Op\n",
      "Adding Add_Op <----- x\n",
      "Appending  Add_Op\n",
      "Pop:  y\n",
      "Adding:  y to the Graph\n",
      "Adding y -----> Add_Op\n",
      "Adding Add_Op <----- y\n",
      "Appending  Add_Op\n",
      "Pop:  z\n",
      "Adding:  z to the Graph\n",
      "Adding z -----> Add_Op\n",
      "Adding Add_Op <----- z\n",
      "Appending  Add_Op\n",
      "Pop:  Add_Op\n",
      "Pop:  Add_Op\n",
      "Pop:  Add_Op\n",
      "Input Nodes:\n",
      "z\n",
      "x\n",
      "y\n",
      "Pop:  z\n",
      "Feeding value:  10  =====>   z\n",
      "Adding  z to the sorted List\n",
      "Removing z -----> Add_Op\n",
      "Removing Add_Op <----- z\n",
      "Pop:  x\n",
      "Feeding value:  4  =====>   x\n",
      "Adding  x to the sorted List\n",
      "Removing x -----> Add_Op\n",
      "Removing Add_Op <----- x\n",
      "Pop:  y\n",
      "Feeding value:  5  =====>   y\n",
      "Adding  y to the sorted List\n",
      "Removing y -----> Add_Op\n",
      "Removing Add_Op <----- y\n",
      "\n",
      "No input nodes!!! Adding:  Add_Op to the Graph\n",
      "\n",
      "Pop:  Add_Op\n",
      "Adding  Add_Op to the sorted List\n",
      "Sorted Nodes:\n",
      "\n",
      "z\n",
      "x\n",
      "y\n",
      "Add_Op\n",
      "<------------------------------------ topological_sort\n",
      "\n",
      "----->Forward pass @  z\n",
      "\n",
      "----->Forward pass @  x\n",
      "\n",
      "----->Forward pass @  y\n",
      "Initial value of Add_Op is 0\n",
      "x:4 ---> Add_Op:4\n",
      "Initial value of Add_Op is 4\n",
      "y:5 ---> Add_Op:9\n",
      "Initial value of Add_Op is 9\n",
      "z:10 ---> Add_Op:19\n",
      "4 + 5 + 10 = 19 (according to miniflow)\n"
     ]
    }
   ],
   "source": [
    "x, y, z = Input('x'), Input('y'), Input('z')\n",
    "\n",
    "f = Add(x, y, z)\n",
    "feed_dict = {x: 4, y: 5, z: 10}\n",
    "graph = topological_sort(feed_dict)\n",
    "addition_res = forward_pass(f, graph)\n",
    "# should output 19\n",
    "print(\"{} + {} + {} = {} (according to miniflow)\".format(feed_dict[x], feed_dict[y], feed_dict[z], addition_res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T_DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Mul(Node):\n",
    "    def __init__(self, *inputs):\n",
    "        Node.__init__(self, inputs)\n",
    "        self.name = \"Mul_Op\"\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        For reference, here's the old way from the last\n",
    "        quiz. You'll want to write code here.\n",
    "        \"\"\"\n",
    "        self.value = 1\n",
    "        for i in range(len(self.inbound_nodes)):\n",
    "            if(DEBUG) : print(\"Initial value of {} is {}\".format(self.name, self.value))\n",
    "                \n",
    "            self.value *=  self.inbound_nodes[i].value\n",
    "            \n",
    "            if(DEBUG) : print(\"{}:{} ---> {}:{}\".format(self.inbound_nodes[i].name, self.inbound_nodes[i].value, \n",
    "                                           self.name, self.value))\n",
    "            \n",
    "        # x_value = self.inbound_nodes[0].value\n",
    "        # y_value = self.inbound_nodes[1].value\n",
    "        # self.value = x_value + y_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----->Forward pass @  z\n",
      "\n",
      "----->Forward pass @  x\n",
      "\n",
      "----->Forward pass @  y\n",
      "Initial value of Mul_Op is 1\n",
      "x:4 ---> Mul_Op:4\n",
      "Initial value of Mul_Op is 4\n",
      "y:5 ---> Mul_Op:20\n",
      "Initial value of Mul_Op is 20\n",
      "z:10 ---> Mul_Op:200\n",
      "\n",
      "4 * 5 * 10 = 200 (according to miniflow)\n"
     ]
    }
   ],
   "source": [
    "x, y, z = Input('x'), Input('y'), Input('z')\n",
    "f1 = Mul(x, y, z)\n",
    "feed_dict = {x: 4, y: 5, z: 10}\n",
    "graph = topological_sort(feed_dict)\n",
    "product = forward_pass(f1, graph)\n",
    "# should output 19\n",
    "print(\"\\n{} * {} * {} = {} (according to miniflow)\".format(feed_dict[x], feed_dict[y], feed_dict[z], product))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Transformation\n",
    "A simple artificial neuron depends on three components:\n",
    "\n",
    "- inputs, $x_i$\n",
    "- weights, $w_i$ \n",
    "- bias, $b$\n",
    "\n",
    "The output, $y$, is just the weighted sum of the inputs plus the bias.\n",
    "\n",
    "$$y =\\sum_i x_iw_i + b$$\n",
    "\n",
    "Gradient:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial X} = W$$\n",
    "$$\\frac{\\partial y}{\\partial W} = X$$\n",
    "$$\\frac{\\partial y}{\\partial b} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![drawing](https://algebra1course.files.wordpress.com/2013/02/slide11.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 22],\n",
       "       [43, 50]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2],[3,4]])\n",
    "b = np.array([[5,6],[7,8]])\n",
    "np.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Node) constructor. Weights and bias\n",
    "        # are treated like inbound nodes.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "        self.name = \"Linear_OP\"\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        self.X = self.inbound_nodes[0]\n",
    "        self.W = self.inbound_nodes[1]\n",
    "        self.b = self.inbound_nodes[2]\n",
    "\n",
    "        self.value = np.dot(self.X.value,self.W.value) + self.b.value\n",
    "        \n",
    "\n",
    "        \n",
    "        if(DEBUG) :  print(\"\\n----->Forward pass @ \" ,self.name)\n",
    "        if(DEBUG) : print(\"{}:\\n{} * \\n{}:\\n{} + \\n{}:\\n{} =\\n {}:\\n{}\".format(self.X.name,self.X.value,\n",
    "                                                                  self.W.name, self.W.value,\n",
    "                                                                  self.b.name, self.b.value,\n",
    "                                                                  self.name, self.value))\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        \n",
    "        if(DEBUG) : print('\\n')\n",
    "        if(DEBUG) : print('=============================\\n\\tBP @ Linear\\n=============================\\n')\n",
    "        if(DEBUG) : print('Initial Gradients:\\n------------------')\n",
    "        if(DEBUG) : print('W.r.t {}: \\n---------------\\n{}'.format(self.X.name, self.gradients[self.X]))\n",
    "        if(DEBUG) : print('W.r.t {}: \\n---------------\\n{}'.format(self.W.name, self.gradients[self.W]))\n",
    "        if(DEBUG) : print('W.r.t {}: \\n---------------\\n{}'.format(self.b.name, self.gradients[self.b]))\n",
    "            \n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            # The out is mostly only one node, a activation function!(sigmoid here)\n",
    "            grad_cost = n.gradients[self]\n",
    "            \n",
    "            if(DEBUG) : print('\\n')\n",
    "            if(DEBUG) : print('Getting ', n.name, 'gradient is : \\n<-----------------------------\\n', grad_cost)\n",
    "            if(DEBUG) : print('\\n')\n",
    "                \n",
    "            # Get the gradient for this node from next node and respective operation \n",
    "            # (mutliply/add) with each input of this node to set their respective gradients\n",
    "            # Set the partial of the loss with respect to this node's inputs.\n",
    "            self.gradients[self.X] += np.dot(grad_cost, self.W.value.T)\n",
    "            # Set the partial of the loss with respect to this node's weights.\n",
    "            self.gradients[self.W] += np.dot(self.X.value.T, grad_cost)\n",
    "            # Set the partial of the loss with respect to this node's bias.\n",
    "            self.gradients[self.b] += np.sum(grad_cost, axis=0, keepdims=False)\n",
    "            \n",
    "        if(DEBUG) : print('Calculated Final Gradient:\\n----------------')\n",
    "        if(DEBUG) : print('W.r.t ',self.X.name,': \\n-------------\\n', self.gradients[self.inbound_nodes[0]])\n",
    "        if(DEBUG) : print('W.r.t ',self.W.name,': \\n-------------\\n', self.gradients[self.inbound_nodes[1]])\n",
    "        if(DEBUG) : print('W.r.t ',self.b.name,': \\n-------------\\n', self.gradients[self.inbound_nodes[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](w2-backprop-graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----->Forward pass @  X\n",
      "\n",
      "----->Forward pass @  W\n",
      "\n",
      "----->Forward pass @  b\n",
      "\n",
      "----->Forward pass @  Linear_OP\n",
      "X:\n",
      "[[ 1.  2.]\n",
      " [ 3.  4.]] * \n",
      "W:\n",
      "[[ 5.  6.]\n",
      " [ 7.  8.]] + \n",
      "b:\n",
      "[-1. -1.] =\n",
      " Linear_OP:\n",
      "[[ 18.  21.]\n",
      " [ 42.  49.]]\n",
      "\n",
      "\n",
      "output: \n",
      " [[ 18.  21.]\n",
      " [ 42.  49.]]\n"
     ]
    }
   ],
   "source": [
    "X, W, b = Input('X'), Input('W'), Input('b')\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "\n",
    "X_ = np.array([[1., 2.], [3., 4.]])\n",
    "W_ = np.array([[5., 6.], [7., 8.]])\n",
    "b_ = np.array([-1., -1])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "print(\"\\n\\noutput: \\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "\n",
    "https://en.wikipedia.org/wiki/Sigmoid_function\n",
    "\n",
    "$$ S(x) = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "Gradient: $\\frac{\\partial S(x)}{\\partial x} = S(x) * (1-S(x))$\n",
    "\n",
    "$\\frac{\\partial cost}{\\partial S(x)} = grad\\_cost$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        # The base class constructor.\n",
    "        Node.__init__(self, [node])\n",
    "        self.name = \"Sigmoid_Op\"\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        if(DEBUG) : print(\"\\n----->Forward pass @ \" ,self.name)\n",
    "        if(DEBUG) : print(\"Initial value of {} is {}\".format(self.name, self.value))\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "        if(DEBUG) : print(\"{}:\\n{} ---> {}:\\n{}\".format(self.inbound_nodes[0].name, self.inbound_nodes[0].value, \n",
    "                                           self.name, self.value))\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        # Initialize the gradients to 0.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "\n",
    "        if(DEBUG) : print('\\n')\n",
    "        if(DEBUG) : print('=============================\\n\\tBP @ Sigmoid\\n=============================\\n')\n",
    "        if(DEBUG) : print('Initial Gradients:\\n------------------')\n",
    "        if(DEBUG) : print('W.r.t ', self.inbound_nodes[0].name, ': \\n----------------\\n', self.gradients[self.inbound_nodes[0]])\n",
    "        \n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self] #For eg. get it from MSE\n",
    "            \n",
    "            if(DEBUG) : print('\\n')\n",
    "            if(DEBUG) : print('Getting ', n.name, 'gradient : \\n<-----------------------------\\n', grad_cost)\n",
    "            if(DEBUG) : print('\\n')\n",
    "                \n",
    "            sigmoid = self.value\n",
    "            self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
    "            \n",
    "        if(DEBUG) : print('Calculated Final Gradient:')\n",
    "        if(DEBUG) : print('--------------------------')\n",
    "        if(DEBUG) : print('W.r.t ',self.inbound_nodes[0].name,': \\n-------------\\n', self.gradients[self.inbound_nodes[0]])    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----->Forward pass @  b\n",
      "\n",
      "----->Forward pass @  X\n",
      "\n",
      "----->Forward pass @  W\n",
      "\n",
      "----->Forward pass @  Linear_OP\n",
      "X:\n",
      "[[-1. -2.]\n",
      " [-1. -2.]] * \n",
      "W:\n",
      "[[ 2. -3.]\n",
      " [ 2. -3.]] + \n",
      "b:\n",
      "[-3. -5.] =\n",
      " Linear_OP:\n",
      "[[-9.  4.]\n",
      " [-9.  4.]]\n",
      "\n",
      "----->Forward pass @  Sigmoid_Op\n",
      "Initial value of Sigmoid_Op is None\n",
      "Linear_OP:\n",
      "[[-9.  4.]\n",
      " [-9.  4.]] ---> Sigmoid_Op:\n",
      "[[  1.23394576e-04   9.82013790e-01]\n",
      " [  1.23394576e-04   9.82013790e-01]]\n",
      "\n",
      "\n",
      "output: \n",
      " [[  1.23394576e-04   9.82013790e-01]\n",
      " [  1.23394576e-04   9.82013790e-01]]\n"
     ]
    }
   ],
   "source": [
    "X, W, b = Input('X'), Input('W'), Input('b')\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "g = Sigmoid(f)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(g, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[  1.23394576e-04   9.82013790e-01]\n",
    " [  1.23394576e-04   9.82013790e-01]]\n",
    "\"\"\"\n",
    "print(\"\\n\\noutput: \\n\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE (Cost/Loss)\n",
    "- (https://en.wikipedia.org/wiki/Linear_regression)\n",
    "- http://setosa.io/ev/ordinary-least-squares-regression/\n",
    "\n",
    "$MSE(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\left (f(y_i|\\theta)-a_i \\right )^2$\n",
    "\n",
    "$f(y_i|\\theta)$ is a function that calculates $y_i$ with parameters $\\theta$ or weights, what it had learned already\n",
    "\n",
    "Gradient:\n",
    "\n",
    "With respect to y: $\\frac{2}{N}(y-a)$   \n",
    "With respect to a: $\\frac{-2}{N}(y-a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "        self.name = \"MSE_Op\"\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        if(DEBUG) : print(\"\\n----->Forward pass @ \" ,self.name)\n",
    "        if(DEBUG) : print(\"Initial value of {} is {}\".format(self.name, self.value))\n",
    "            \n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "\n",
    "        self.m = self.inbound_nodes[0].value.shape[0]\n",
    "        # Save the computed output for backward.\n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(np.square(self.diff))\n",
    "        \n",
    "        if(DEBUG) : print(\"{}:\\n{} - \\n{}:\\n{} =\\n {}:\\n{}\".format(self.inbound_nodes[0].name,y,\n",
    "                                                                  self.inbound_nodes[1].name, a,\n",
    "                                                                  self.name, self.value))\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the cost.\n",
    "\n",
    "        This is the final node of the network so outbound nodes\n",
    "        are not a concern.\n",
    "        \"\"\"\n",
    "        if(DEBUG) : print('\\n')\n",
    "        if(DEBUG) : print('=============================\\n\\tBP @ MSE\\n=============================\\n')\n",
    "        if(DEBUG) : print('Initial Gradients:\\n------------------')\n",
    "        if(DEBUG) : print('Nothing! Since this node will be the last node!!!\\n')\n",
    "        \n",
    "        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff #for eg. this goes back to Sigmoid\n",
    "        \n",
    "        if(DEBUG) : print('Calculated Final Gradient:\\n----------------')\n",
    "        if(DEBUG) : print('W.r.t ',self.inbound_nodes[0].name,': \\n------------------\\n', self.gradients[self.inbound_nodes[0]])\n",
    "        if(DEBUG) : print('W.r.t ',self.inbound_nodes[1].name,': \\n------------------\\n', self.gradients[self.inbound_nodes[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----->Forward pass @  y\n",
      "\n",
      "----->Forward pass @  a\n",
      "\n",
      "----->Forward pass @  MSE_Op\n",
      "Initial value of MSE_Op is None\n",
      "y:\n",
      "[[1]\n",
      " [2]\n",
      " [3]] - \n",
      "a:\n",
      "[[  4.5]\n",
      " [  5. ]\n",
      " [ 10. ]] =\n",
      " MSE_Op:\n",
      "23.416666666666668\n",
      "23.4166666667\n"
     ]
    }
   ],
   "source": [
    "y, a = Input('y'), Input('a')\n",
    "cost = MSE(y, a)\n",
    "\n",
    "y_ = np.array([1, 2, 3])\n",
    "a_ = np.array([4.5, 5, 10])\n",
    "\n",
    "feed_dict = {y: y_, a: a_}\n",
    "graph = topological_sort(feed_dict)\n",
    "# forward pass\n",
    "# forward_pass_mse(graph)\n",
    "forward_pass(cost, graph)\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "23.4166666667\n",
    "\"\"\"\n",
    "print(cost.value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Backpropagation\n",
    "- Partial Differentiation\n",
    "    - https://math.stackexchange.com/questions/45952/why-do-we-take-a-derivative\n",
    "    - https://simple.wikipedia.org/wiki/Derivative_(mathematics)\n",
    "    - https://en.wikipedia.org/wiki/Partial_derivative\n",
    "    - https://en.wikipedia.org/wiki/Chain_rule\n",
    "\n",
    "    \n",
    "Khan Academy:    \n",
    "- https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivatives/v/partial-derivatives-introduction   \n",
    "- https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient     \n",
    "- https://www.khanacademy.org/math/ap-calculus-ab/ab-derivative-rules/ab-chain-rule/v/chain-rule-introduction   \n",
    "\n",
    "Rerferences:\n",
    "- http://cs231n.github.io/optimization-2/\n",
    "- https://en.wikipedia.org/wiki/Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ASCII Art:\n",
    "\n",
    "```\n",
    "  -2\n",
    "  x--------\n",
    "  -4       \\ \n",
    "            \\  -----  q 3\n",
    "              |  +  | -----\n",
    "            /  -----   -4   \\\n",
    "   5       /                 \\\n",
    "  y--------                   \\  -----  -12\n",
    "   -4                           |  *  | ----- f(x,y,z) = (x+y)z\n",
    "                              /  -----    1\n",
    "                             /\n",
    "   -4                       /  \n",
    "  z-------------------------\n",
    "    3\n",
    "```   \n",
    "\n",
    "\n",
    "Let $f(x,y,z) = (x+y)z$ be some function that calculates some error.\n",
    "\n",
    "$Eg.\\ values: x = -2, y = 5, z = -4$\n",
    "\n",
    "$\n",
    "Let\\ q = x + y \\\\\n",
    "Partial\\ derivatives...\\\\\n",
    "\\frac{\\partial q}{\\partial x} = 1 \\\\\n",
    "\\frac{\\partial q}{\\partial y} = 1\n",
    "$\n",
    "\n",
    "$\n",
    "Now\\ f = qz \\\\\n",
    "Partial\\ derivatives...\\\\\n",
    "\\frac{\\partial f}{\\partial q} = z = -4 \\\\\n",
    "\\frac{\\partial f}{\\partial z} = q = 3\n",
    "$\n",
    "\n",
    "How much does each of x,y,z contributed to f,\n",
    "\n",
    "i.e\n",
    "$\n",
    "\\frac{\\partial f}{\\partial x} \\\n",
    "\\frac{\\partial f}{\\partial y} \\\n",
    "\\frac{\\partial f}{\\partial z} \n",
    "$\n",
    "\n",
    "We know that $\\frac{\\partial f} {\\partial f} = 1$\n",
    "\n",
    "Chain rule:  \n",
    "$\\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial y} = -4 . 1 = -4$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x} = -4 . 1 = -4$\n",
    "\n",
    "\n",
    "![](gradients_in_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# **Another Example **\n",
    "# ![](back_prop_example1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ![](back_prop_example1_sol.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass and a backward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a network with a linear node $l_1$, a sigmoid node $s$, and another linear node $l_2$, followed by an MSE node to calculate the cost, $C$.\n",
    "\n",
    "![](two-layer-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each of the values of these nodes flows forwards and eventually produces the cost $C$. \n",
    "For example, the value of the second linear node $l_2$ goes into the cost node and determines the value of that node. Accordingly, a change in $l_2$ will produce a change in $C$. We can write this relationship between the changes as a gradient,\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial l_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what a gradient means, it's a slope, how much you change the cost $\\partial C$ given a change in $l_2$, $\\partial l_2$. So a node with a larger gradient with respect to the cost is going to contribute a larger change to the cost. In this way, we can assign blame for the cost to each node. The larger the gradient for a node, the more blame it gets for the final cost. And the more blame a node has, the more we'll update it in the gradient descent step.\n",
    "\n",
    "If we want to update one of the weights with gradient descent, we'll need the gradient of the cost with respect to those weights. Let's see how we can use this framework to find the gradient for the weights in the second layer, $w_2$. We want to calculate the gradient of $C$ with respect to $w_2$:\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w_2}$\n",
    "\n",
    "We can see in the graph that $w_2$ is connected to $l_2$, so a change in $w_2$ is going to create a change in $l_2$ which then creates a change in $C$. We can assign blame to $w_2$ by sending the cost gradient back through the network. First you have how much $l_2$ affected $C$, then how much $w_2$ affected $l_2$. Multiplying these gradients together gets you the total blame attributed to $w_2$.\n",
    "\n",
    "![](w2-backprop-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying these gradients is just an application of the chain rule:\n",
    "$$\\frac{\\partial C}{\\partial w_2} = \\frac{\\partial C}{\\partial l_2} * \\frac{\\partial l_2}{\\partial w_2}$$\n",
    "\n",
    "\n",
    "You can see in the graph $w_2$, $l_2$, and $C$ are chained together. Any change in $w_2$ will create a change in $l_2$ and the size of that change is given by the gradient $∂l_2/∂w_2$. Now, since $l_2$ is changing this will cause a change in the cost $C$ and the size of that change is given by the gradient $∂C/∂l_2$. You can think of the chain rule similarly to the domino effect, changing something in the network will propagate through it altering other nodes along the way.\n",
    "If you think of the chain rule as normal fractions, you can see that $∂l_2$ in the denominator and numerator cancel out and you get back $∂C/∂w_2$ (although it doesn't exactly work like normal fractions, but it helps to keep track of things.) Okay, let's work out the gradient for $w_2$. First, we need to know the gradient for $l_2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost $C = \\frac{1}{m} \\sum_x (y(x) - l_2)^2$\n",
    "\n",
    "And the value for the second linear node is $l_2 = w_2 . s + b_2$\n",
    "\n",
    "where $w_2$, $s$, and $b_2$ are all vectors and $w_2 . s$ means the dot product of $w_2$ and $s$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial l_2} = \\frac{\\partial}{\\partial l_2}\\Bigg[ \\frac{1}{m} \\sum_x (y(x) - l_2)^2\\Bigg]\n",
    "= \\frac{-2}{m}\\sum_x (y(x) - l_2)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial l_2}{\\partial w_2} = \\frac{\\partial}{\\partial w_2}\\bigg[ w_2 . s + b_2\\bigg] = s\n",
    "$$\n",
    "\n",
    "And putting these together, you get the gradient for $w_2$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_2} = \\frac{-2}{m}\\sum_x (y(x) - l_2) s\n",
    "$$\n",
    "\n",
    "This is the gradient you use in the gradient descent update for $w_2$. You can see what we did here, we walked back through the graph and multiplied all the gradients we found along the way.\n",
    "Now, let's go deeper and calculate the gradient for $w_1$. Here we use the same method as before, walking backwards through the graph.\n",
    "\n",
    "![](w1-backprop-graph.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully it's clear now how to write out the gradient for $w_1$ just by looking at the graph. Using the chain rule, we'll write out the gradients for each node going backwards through the graph until we get to $w_1$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_1} = \\frac{\\partial C}{\\partial l_2} \\frac{\\partial l_2}{\\partial s} \\frac{\\partial s}{\\partial l_1} \\frac{\\partial l_1}{\\partial w_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start calculating each gradient in this expression to get the gradient for $w_1$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l_2}{\\partial s} = \\frac{\\partial }{\\partial s} \\bigg[ w_2 . s + b_2 \\bigg] = w2\n",
    "$$\n",
    "\n",
    "The next part is the gradient of the sigmoid function, $s=f(l_1)$. Since we're using the logistic function here, the derivative can be written in terms of the sigmoid itself\n",
    "\n",
    "$$\n",
    "\\frac{\\partial s}{\\partial l_1} = \\frac{\\partial }{\\partial l_1}f(l_1)\n",
    "= f(l_1)(1-f(l_1))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l_1}{\\partial w_1} = \\frac{\\partial }{\\partial w_1} \\bigg[ w_1 . x + b_1 \\bigg] = x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this all together, you get\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_1} = \\frac{-2}{m}\\sum_x (y(x) - l_2) . w2 . f(l_1). (1-f(l_1)) . x\n",
    "$$\n",
    "\n",
    "Now we can see a clear pattern. To find the gradient, you just multiply the gradients for all nodes in front of it going backwards from the cost. This is the idea behind backpropagation. The gradients are passed backwards through the network and used with gradient descent to update the weights and biases. If a node has multiple outgoing nodes, you just sum up the gradients from each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T_DEBUG = False\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----->Forward pass @  W1\n",
      "\n",
      "----->Forward pass @  X\n",
      "\n",
      "----->Forward pass @  y\n",
      "\n",
      "----->Forward pass @  b\n",
      "\n",
      "----->Forward pass @  Linear_OP\n",
      "X:\n",
      "[[-1. -2.]\n",
      " [-1. -2.]] * \n",
      "W1:\n",
      "[[ 2.]\n",
      " [ 3.]] + \n",
      "b:\n",
      "[-3.] =\n",
      " Linear_OP:\n",
      "[[-11.]\n",
      " [-11.]]\n",
      "\n",
      "----->Forward pass @  Sigmoid_Op\n",
      "Initial value of Sigmoid_Op is None\n",
      "Linear_OP:\n",
      "[[-11.]\n",
      " [-11.]] ---> Sigmoid_Op:\n",
      "[[  1.67014218e-05]\n",
      " [  1.67014218e-05]]\n",
      "\n",
      "----->Forward pass @  MSE_Op\n",
      "Initial value of MSE_Op is None\n",
      "y:\n",
      "[[1]\n",
      " [2]] - \n",
      "Sigmoid_Op:\n",
      "[[  1.67014218e-05]\n",
      " [  1.67014218e-05]] =\n",
      " MSE_Op:\n",
      "2.4999498960133932\n",
      "\n",
      "\n",
      "=============================\n",
      "\tBP @ MSE\n",
      "=============================\n",
      "\n",
      "Initial Gradients:\n",
      "------------------\n",
      "Nothing! Since this node will be the last node!!!\n",
      "\n",
      "Calculated Final Gradient:\n",
      "----------------\n",
      "W.r.t  y : \n",
      "------------------\n",
      " [[ 0.9999833]\n",
      " [ 1.9999833]]\n",
      "W.r.t  Sigmoid_Op : \n",
      "------------------\n",
      " [[-0.9999833]\n",
      " [-1.9999833]]\n",
      "\n",
      "\n",
      "=============================\n",
      "\tBP @ Sigmoid\n",
      "=============================\n",
      "\n",
      "Initial Gradients:\n",
      "------------------\n",
      "W.r.t  Linear_OP : \n",
      "----------------\n",
      " [[ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "\n",
      "Getting  MSE_Op gradient : \n",
      "<-----------------------------\n",
      " [[-0.9999833]\n",
      " [-1.9999833]]\n",
      "\n",
      "\n",
      "Calculated Final Gradient:\n",
      "--------------------------\n",
      "W.r.t  Linear_OP : \n",
      "-------------\n",
      " [[ -1.67008640e-05]\n",
      " [ -3.34020069e-05]]\n",
      "\n",
      "\n",
      "=============================\n",
      "\tBP @ Linear\n",
      "=============================\n",
      "\n",
      "Initial Gradients:\n",
      "------------------\n",
      "W.r.t X: \n",
      "---------------\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "W.r.t W1: \n",
      "---------------\n",
      "[[ 0.]\n",
      " [ 0.]]\n",
      "W.r.t b: \n",
      "---------------\n",
      "[ 0.]\n",
      "\n",
      "\n",
      "Getting  Sigmoid_Op gradient is : \n",
      "<-----------------------------\n",
      " [[ -1.67008640e-05]\n",
      " [ -3.34020069e-05]]\n",
      "\n",
      "\n",
      "Calculated Final Gradient:\n",
      "----------------\n",
      "W.r.t  X : \n",
      "-------------\n",
      " [[ -3.34017280e-05  -5.01025919e-05]\n",
      " [ -6.68040138e-05  -1.00206021e-04]]\n",
      "W.r.t  W1 : \n",
      "-------------\n",
      " [[  5.01028709e-05]\n",
      " [  1.00205742e-04]]\n",
      "W.r.t  b : \n",
      "-------------\n",
      " [ -5.01028709e-05]\n",
      "\n",
      "\n",
      "=============================\n",
      "\tBP @ b\n",
      "=============================\n",
      "\n",
      "Initial Gradients:\n",
      "------------------\n",
      "W.r.t b: \n",
      "------------\n",
      "0\n",
      "\n",
      "\n",
      "Getting  Linear_OP gradient : \n",
      "<-----------------------------\n",
      " [ -5.01028709e-05]\n",
      "\n",
      "\n",
      "Calculated Final Gradient:(Note: Calculated by next node in the graph!!!)\n",
      "----------------\n",
      "W.r.t  b  : \n",
      "-------------\n",
      " [ -5.01028709e-05]\n",
      "\n",
      "\n",
      "=============================\n",
      "\tBP @ y\n",
      "=============================\n",
      "\n",
      "Initial Gradients:\n",
      "------------------\n",
      "W.r.t y: \n",
      "------------\n",
      "0\n",
      "\n",
      "\n",
      "Getting  MSE_Op gradient : \n",
      "<-----------------------------\n",
      " [[ 0.9999833]\n",
      " [ 1.9999833]]\n",
      "\n",
      "\n",
      "Calculated Final Gradient:(Note: Calculated by next node in the graph!!!)\n",
      "----------------\n",
      "W.r.t  y  : \n",
      "-------------\n",
      " [[ 0.9999833]\n",
      " [ 1.9999833]]\n",
      "\n",
      "\n",
      "=============================\n",
      "\tBP @ X\n",
      "=============================\n",
      "\n",
      "Initial Gradients:\n",
      "------------------\n",
      "W.r.t X: \n",
      "------------\n",
      "0\n",
      "\n",
      "\n",
      "Getting  Linear_OP gradient : \n",
      "<-----------------------------\n",
      " [[ -3.34017280e-05  -5.01025919e-05]\n",
      " [ -6.68040138e-05  -1.00206021e-04]]\n",
      "\n",
      "\n",
      "Calculated Final Gradient:(Note: Calculated by next node in the graph!!!)\n",
      "----------------\n",
      "W.r.t  X  : \n",
      "-------------\n",
      " [[ -3.34017280e-05  -5.01025919e-05]\n",
      " [ -6.68040138e-05  -1.00206021e-04]]\n",
      "\n",
      "\n",
      "=============================\n",
      "\tBP @ W1\n",
      "=============================\n",
      "\n",
      "Initial Gradients:\n",
      "------------------\n",
      "W.r.t W1: \n",
      "------------\n",
      "0\n",
      "\n",
      "\n",
      "Getting  Linear_OP gradient : \n",
      "<-----------------------------\n",
      " [[  5.01028709e-05]\n",
      " [  1.00205742e-04]]\n",
      "\n",
      "\n",
      "Calculated Final Gradient:(Note: Calculated by next node in the graph!!!)\n",
      "----------------\n",
      "W.r.t  W1  : \n",
      "-------------\n",
      " [[  5.01028709e-05]\n",
      " [  1.00205742e-04]]\n",
      "\n",
      "\n",
      " [array([[ -3.34017280e-05,  -5.01025919e-05],\n",
      "       [ -6.68040138e-05,  -1.00206021e-04]]), array([[ 0.9999833],\n",
      "       [ 1.9999833]]), array([[  5.01028709e-05],\n",
      "       [  1.00205742e-04]]), array([ -5.01028709e-05])]\n"
     ]
    }
   ],
   "source": [
    "#Placeholders\n",
    "X = Input('X') # 2 x 2\n",
    "y = Input('y') # 2 x 1\n",
    "W1 = Input('W1') # 2 x 1\n",
    "b = Input('b') # 1 x 1\n",
    "\n",
    "#Graph Operations\n",
    "f1 = Linear(X, W1, b) # 2 x 2 . 2 x 1 + 1 x 1 = 2 x 1 \n",
    "activation_output = Sigmoid(f1) # 2 x 1\n",
    "\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W1_ = np.array([[2.], [3.]])\n",
    "b_ = np.array([-3.])\n",
    "y_ = np.array([1, 2])\n",
    "\n",
    "if True: #For simple graph\n",
    "    cost = MSE(y, activation_output) # 2 x 1\n",
    "    feed_dict = {\n",
    "        X: X_,\n",
    "        y: y_,\n",
    "        W1: W1_,\n",
    "        b: b_,\n",
    "    }\n",
    "    graph = topological_sort(feed_dict)\n",
    "    forward_and_backward(graph)\n",
    "    # return the gradients for each Input\n",
    "    gradients = [t.gradients[t] for t in [X, y, W1, b]]\n",
    "else: #For the once explained above!\n",
    "    W2 = Input('W2') \n",
    "    W2_ = np.array([[2.]]) # 1 x 1\n",
    "    b2 = Input('b2') \n",
    "    b2_ = np.array([-3.]) # 1,\n",
    "    \n",
    "    f2 = Linear(activation_output, W2, b2) # 2 x 1 . 1 x 1 + 1 x 1 = 2 x 1\n",
    "    cost = MSE(y, f2) # 2 x 1 - 2 x 1\n",
    "\n",
    "    feed_dict = {\n",
    "        X: X_,\n",
    "        y: y_,\n",
    "        W1: W1_,\n",
    "        b: b_,\n",
    "        W2: W2_,\n",
    "        b2: b2_\n",
    "    }\n",
    "    graph = topological_sort(feed_dict)\n",
    "    forward_and_backward(graph)\n",
    "    # return the gradients for each Input\n",
    "    gradients = [t.gradients[t] for t in [X, y, W1, b, W2, b2]]\n",
    " \n",
    "\"\"\"\n",
    "Expected output for case 1:\n",
    "\n",
    "[array([[ -3.34017280e-05,  -5.01025919e-05],\n",
    "       [ -6.68040138e-05,  -1.00206021e-04]]), \n",
    " array([[ 0.9999833],\n",
    "       [ 1.9999833]]), \n",
    " array([[  5.01028709e-05],\n",
    "       [  1.00205742e-04]]), \n",
    " array([ -5.01028709e-05])\n",
    "]\n",
    "\"\"\"\n",
    "print('\\n\\n', gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SGD with Boston Dataset\n",
    "- https://www.youtube.com/watch?v=GCvWD9zIF-s\n",
    "- https://imgur.com/SmDARzn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)\n",
    "$$x = x - learning\\ rate * \\frac{\\partial cost}{\\partial x}$$\n",
    "\n",
    "\n",
    "Stochastic Gradient Descent\n",
    "Stochastic Gradient Descent (SGD) is a version of Gradient Descent where on each forward pass a batch of data is randomly sampled from total dataset. Remember when we talked about the batch size earlier? That's the size of the batch. Ideally, the entire dataset would be fed into the neural network on each forward pass, but in practice, it's not practical due to memory constraints. SGD is an approximation of Gradient Descent, the more batches processed by the neural network, the better the approximation.\n",
    "\n",
    "A naïve implementation of SGD involves:\n",
    "\n",
    "1. Randomly sample a batch of data from the total dataset.\n",
    "2. Running the network forward and backward to calculate the gradient (with data from (1)).\n",
    "3. Apply the gradient descent update.\n",
    "4. Repeat steps 1-3 until convergence or the loop is stopped by another mechanism (i.e. the number of epochs).\n",
    "\n",
    "\n",
    "If all goes well, the network's loss should generally trend downwards, indicating more useful weights and biases over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x)=x^4−3x^3+2$$\n",
    "\n",
    "And its derivative\n",
    "$$f'(x)=4x^3−9x^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The local minimum occurs at 2.249965\n"
     ]
    }
   ],
   "source": [
    "# The gradient descent algorithm is applied to find a local minimum of the function f(x)=x^4−3x^3+2, with derivative\n",
    "#f'(x)=4x^3−9x^2. Here is an implementation in the Python programming language.\n",
    "# From calculation, it is expected that the local minimum occurs at x=9/4\n",
    "\n",
    "cur_x = 6 # The algorithm starts at x=6\n",
    "gamma = 0.01 # step size multiplier\n",
    "precision = 0.00001\n",
    "previous_step_size = cur_x\n",
    "\n",
    "def df(x):\n",
    "    return 4 * x**3 - 9 * x**2\n",
    "\n",
    "while previous_step_size > precision:\n",
    "    prev_x = cur_x\n",
    "    cur_x += -gamma * df(prev_x)\n",
    "    previous_step_size = abs(cur_x - prev_x)\n",
    "\n",
    "print(\"The local minimum occurs at %f\" % cur_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    \"\"\"\n",
    "    Updates the value of each trainable with SGD.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `trainables`: A list of `Input` Nodes representing weights/biases.\n",
    "        `learning_rate`: The learning rate.\n",
    "    \"\"\"\n",
    "    # Performs SGD\n",
    "    #\n",
    "    # Loop over the trainables\n",
    "    for t in trainables:\n",
    "        # Change the trainable's value by subtracting the learning rate\n",
    "        # multiplied by the partial of the cost with respect to this\n",
    "        # trainable.\n",
    "        partial = t.gradients[t]\n",
    "        t.value -= learning_rate * partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T_DEBUG = False\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Have fun with the number of epochs!\n",
    "\n",
    "Be warned that if you increase them too much,\n",
    "logs will overshoot :)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "# from miniflow import *\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1] # 13\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 100\n",
    "# Total number of examples\n",
    "m = X_.shape[0] # 506\n",
    "batch_size = 11\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "total_loss = []\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        forward_and_backward(graph)\n",
    "\n",
    "        # Step 3\n",
    "        sgd_update(trainables)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "        \n",
    "\n",
    "#     print(\"=======> Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
    "    total_loss.append(loss/steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f52adb06550>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9CIiAaegFxEJOAW9iEjA/X+HgBT5ceEhMgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f52ad5fd518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot.plot(range(len(total_loss)), total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
